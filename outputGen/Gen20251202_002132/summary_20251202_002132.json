{
  "GEN_GPT4oMini": {
    "status": "ok",
    "output_dir": "results/generation/Gen20251202_002132/GEN_GPT4oMini_20251202_002132",
    "excel": "results/generation/Gen20251202_002132/GEN_GPT4oMini_20251202_002132/GEN_GPT4oMini_20251202_002132.xlsx",
    "scores": {
      "context_precision": 1.0,
      "context_recall": 1.0,
      "answer_relevancy": 0.435,
      "faithfulness": 0.5,
      "answer_correctness": 0.536
    }
  },
  "GEN_GEMMA_7B": {
    "status": "ok",
    "output_dir": "results/generation/Gen20251202_002132/GEN_GEMMA_7B_20251202_002132",
    "excel": "results/generation/Gen20251202_002132/GEN_GEMMA_7B_20251202_002132/GEN_GEMMA_7B_20251202_002132.xlsx",
    "scores": {
      "context_precision": 1.0,
      "context_recall": 0.938,
      "answer_relevancy": 0.777,
      "faithfulness": 0.534,
      "answer_correctness": 0.572
    }
  },
  "GEN_LLAMA2_CHAT": {
    "status": "ok",
    "output_dir": "results/generation/Gen20251202_002132/GEN_LLAMA2_CHAT_20251202_002132",
    "excel": "results/generation/Gen20251202_002132/GEN_LLAMA2_CHAT_20251202_002132/GEN_LLAMA2_CHAT_20251202_002132.xlsx",
    "scores": {
      "context_precision": 1.0,
      "context_recall": 1.0,
      "answer_relevancy": 0.606,
      "faithfulness": 0.472,
      "answer_correctness": 0.397
    }
  },
  "GEN_PHI3_MEDIUM": {
    "status": "ok",
    "output_dir": "results/generation/Gen20251202_002132/GEN_PHI3_MEDIUM_20251202_002132",
    "excel": "results/generation/Gen20251202_002132/GEN_PHI3_MEDIUM_20251202_002132/GEN_PHI3_MEDIUM_20251202_002132.xlsx",
    "scores": {
      "context_precision": 1.0,
      "context_recall": 0.938,
      "answer_relevancy": 0.704,
      "faithfulness": 0.647,
      "answer_correctness": 0.527
    }
  }
}